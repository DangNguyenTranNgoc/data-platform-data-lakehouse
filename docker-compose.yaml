version: '3.9'
services:

  ###################
  ## Storage Layer ##
  ###################
  postgres:
    hostname: postgres
    image: postgres:15-alpine
    container_name: postgres
    profiles:
      - all
      - core
      - database
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
    healthcheck:
      test: ["CMD", "psql", "-U", "${POSTGRES_USER}", "${POSTGRES_DB}"]
    ports:
      - "5432:5432"
    volumes:
      - ${PWD}/mnt/postgresql:/var/lib/postgresql/data
    networks:
      - platform-network
  
  database-init:
    build:
      context: ./container/database-init
      dockerfile: database-init.dockerfile
    profiles:
      - all
      - core
      - database
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ${PWD}/container/database-init/scripts:/app
    networks:
      - platform-network

  hive-metastore:
    build:
      context: ./containers/hive-metastore/
      dockerfile: hive-metastore.dockerfile
    hostname: hive-metastore
    container_name: hive-metastore
    profiles:
      - all
      - core
    depends_on:
      postgres:
        condition: service_healthy
      database-init:
        condition: service_completed_successfully
    environment:
      - DATABASE_HOST=postgres
      - DATABASE_DB=${POSTGRES_DB}
      - DATABASE_USER=${POSTGRES_USER}
      - DATABASE_PASSWORD=${POSTGRES_PASSWORD}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_ENDPOINT_URL=http://minio:9000
    volumes:
      - ${PWD}/mnt/hive:/user/hive
    ports:
      - "9083:9083"
    networks:
      - platform-network
  
  trino:
    hostname: trino
    image: 'trinodb/trino:latest'
    container_name: trino
    profiles:
      - all
      - core
    ports:
      - "8080:8080"
    volumes:
      - ${PWD}/mnt/trino-etc:/etc/trino
    networks:
      - platform-network
  
  minio:
    user: "${UID}:${GID}"
    hostname: minio
    image: quay.io/minio/minio:latest
    container_name: minio
    profiles:
      - all
      - core
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ${PWD}/mnt/minio:/bitnami/minio/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_PROMETHEUS_AUTH_TYPE: public # Just for dev/staging env
    command: server /data --console-address ":9001"
    networks:
      - platform-network
  
  #####################
  ## Transform Layer ##
  #####################
  dbt-container:
    user: ${UID}:${GID}
    build:
      context: ./containers/dbt-container/
      dockerfile: dbt.dockerfile
    hostname: dbt-container
    container_name: dbt-container
    profiles:
      - all
      - database
    volumes:
      - ${PWD}/mnt/dbt:/app
    networks:
      - platform-network
  
  dbt-spark3-thrift:
    build: docker/
    ports:
      - "10000:10000"
      - "4040:4040"
    depends_on:
      - dbt-hive-metastore
    command: >
      --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
      --name Thrift JDBC/ODBC Server
    volumes:
      - ./.spark-warehouse/:/spark-warehouse/
      - ./docker/hive-site.xml:/usr/spark/conf/hive-site.xml
      - ./docker/spark-defaults.conf:/usr/spark/conf/spark-defaults.conf
    environment:
      - WAIT_FOR=dbt-hive-metastore:5432

  spark-thrift-server:
    build:
      context: ./containers/spark-thrift-server/
      dockerfile: sts.dockerfile
    container_name: spark-thrift-server    
    depends_on:
      - spark-master
      - hive-metastore
    ports:
      - "10000:10000"
      - ""
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1G
      SPARK_DRIVER_MEMORY: 1G
      SPARK_EXECUTOR_MEMORY: 1G
      SPARK_WORKLOAD: thift
      SPARK_LOCAL_IP: spark-thrift-server
      AWS_ACCESS_KEY_ID:  ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      HUDI_S3_BUCKET: ${HUDI_S3_BUCKET}       
    volumes:
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data   
    networks:
      - platform-network

  #########################
  ## Visualization Layer ##
  #########################

  # Metabase

  #########################
  ## Orchestration Layer ##
  #########################

  # Airflow

  #####################
  ## Ingestion Layer ##
  #####################

  # Airbyte

  ######################
  ## Monitoring Layer ##
  ######################
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: cadvisor
    profiles:
      - all
      - monitor
    ports:
      - 8081:8080
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      # This mount is for WSL2, based on: https://github.com/google/cadvisor/issues/2648
      #- /mnt/windows-docker/:/var/lib/docker:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    networks:
      - platform-network
    devices: 
      - /dev/kmsg

  prometheus:
    user: "${UID}:${GID}"
    image: prom/prometheus:latest
    container_name: prometheus
    profiles:
      - all
      - monitor
    ports:
      - 9090:9090
    command:
      - --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - ${PWD}/mnt/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      - cadvisor
    networks:
      - platform-network

  grafana:
    user: "${UID}:${GID}"
    image: grafana/grafana
    container_name: grafana
    profiles:
      - all
      - monitor
    ports:
      - 4000:3000
    volumes:
      - ${PWD}/mnt/grafana/:/var/lib/grafana
      - ${PWD}/mnt/grafana-provisioning/:/etc/grafana/provisioning/
    restart: always
    depends_on:
      - prometheus
    networks:
      - platform-network

networks:
  platform-network:
    driver: bridge
